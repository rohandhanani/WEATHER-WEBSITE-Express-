Breadth first search

def bfs(visited, graph, node):
    visited.append(node)
    queue.append(node)
    while queue:
        v = queue.pop(0)
        print(v)
        for u in graph[v]:
            if u not in visited:
                visited.append(u)
                queue.append(u)

g = {
    '1': ('2', '3'),
    '2': ('4', '5'),
    '3': ('6'),
    '4': (),
    '5': ('6'),
    '6': ()
}

visited = []
queue = []

bfs(visited, g, '1')



-----------------------------------------------------------------------------------------------------------------------------


Depth first search 

def dfs(visited, graph, node):
  if node not in visited:
    print(node)
    visited.add(node)
    for n in graph[node]:
      dfs(visited,graph,n)
      
graph = {
  '1' : ['2','3'],
  '2' : ['4','5'],
  '3' : ['6'],
  '4' : [],
  '5' : ['6'],
  '6' : []
}

visited = set()

dfs(visited, graph, '5')



--------------------------------------------------------------------


Best first search 


from queue import PriorityQueue
v = 14
graph = [[] for _ in range(v)]

def add_edge(x, y, cost):
    graph[x].append((y, cost))
    graph[y].append((x, cost))

add_edge(0, 1, 3)
add_edge(0, 2, 6)
add_edge(0, 3, 5)
add_edge(1, 4, 9)

def best_first_search(src, target, n):
    visited = [False] * n
    pq = PriorityQueue()
    pq.put((0, src))
    visited[src] = True
    
    while not pq.empty():
        u = pq.get()[1]
        print(u)
        
        if u == target:
            break

        for v, c in graph[u]:
            if not visited[v]:
                visited[v] = True
                pq.put((c, v))

source = 0
target = 9

print("Best-First Search Result, starting from vertex 1:")
best_first_search(source, target, v)


---------------------------------------------------------------------------------------------------------------



Linear / decision/ Logistic


import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LinearRegression

df = pd.read_csv("data.csv")
print(df)
df.head

y = df.loc[:,"diagnosis"].values
X = df.drop(["diagnosis","id","Unnamed: 32"],axis=1).values

le = LabelEncoder()
y = le.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y,random_state=0)


------------------------------------------    Logistic ---------------------------------------------------------------


model = LogisticRegression()

model.fit(X_train,y_train)

y_pred = model.predict(X_test)

print(y_pred)

--------------------------------------------  Linear  ---------------------------------------------------

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]

accuracy = accuracy_score(y_test, y_pred_binary)
print(f"Accuracy: {accuracy}")


---------------------------------------------------- Decisiion   ----------------------------------------------------------------------

model = DecisionTreeClassifier(random_state = 42)
model.fit(X_train, y_train)

y_train_pred=model.predict(X_train)
y_test_pred=model.predict(X_test)

tree_train = accuracy_score(y_train, y_train_pred)
tree_test = accuracy_score(y_test, y_test_pred)

print(f'Decision tree train/test accuracies: {tree_train:.3f}/{tree_test:.3f}')


---------------------------------------------------------------------------------------------------------------

K â€“ means

import numpy as nm
import matplotlib.pyplot as mtp
import pandas as pd

dataset = pd.read_csv('/content/Mall_Customers.csv')
print(dataset)
x = dataset.iloc[:, [3, 4]].values

from sklearn.cluster import KMeans
wcss_list= []

for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state= 42)
    kmeans.fit(x)
    wcss_list.append(kmeans.inertia_)
mtp.plot(range(1, 11), wcss_list)
mtp.title('The Elobw Method Graph')
mtp.xlabel('Number of clusters(k)')
mtp.ylabel('wcss_list')
mtp.show()

kmeans = KMeans(n_clusters=5, init='k-means++', random_state= 42)
y_predict= kmeans.fit_predict(x)

mtp.scatter(x[y_predict == 0, 0], x[y_predict == 0, 1], s = 100, c = 'blue', label = 'Cluster 1') 
mtp.scatter(x[y_predict == 1, 0], x[y_predict == 1, 1], s = 100, c = 'green', label = 'Cluster 2') 
mtp.scatter(x[y_predict== 2, 0], x[y_predict == 2, 1], s = 100, c = 'red', label = 'Cluster 3') 
mtp.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroid')
mtp.title('Clusters of customers')
mtp.xlabel('Annual Income (k$)')
mtp.ylabel('Spending Score (1-100)')
mtp.legend()
mtp.show()

